{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)\n",
    "pickle.dump(cases, open(\"cases.bin\", \"wb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "i = 0\n",
    "\n",
    "doc_ids = []\n",
    "brief_titles = []\n",
    "detailed_descriptions = []\n",
    "brief_summaries = []\n",
    "criterias = []\n",
    "genders = []\n",
    "minimum_ages = []\n",
    "maximum_ages = []\n",
    "\n",
    "\n",
    "iterations = 1000\n",
    "count = 0\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "                doc_ids.append(doc_id.text.strip())\n",
    "        \n",
    "        if judged is False:\n",
    "            continue\n",
    "        i = i + 1\n",
    "      \n",
    "        for brief_title in root.iter('brief_title'):\n",
    "            brief_titles.append(brief_title.text.strip())\n",
    "\n",
    "        for detailed_description in root.iter('detailed_description'):\n",
    "            for child in detailed_description:\n",
    "                detailed_descriptions.append(child.text.strip())\n",
    "\n",
    "        for brief_summary in root.iter('brief_summary'):\n",
    "            for child in brief_summary:\n",
    "                brief_summaries.append(child.text.strip())\n",
    "\n",
    "        for criteria in root.iter('criteria'):\n",
    "            for child in criteria:\n",
    "                criterias.append(child.text.strip())\n",
    "\n",
    "        for gender in root.iter('gender'):\n",
    "            genders.append(gender.text.strip())\n",
    "\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            minimum_ages.append(minimum_age.text.strip())\n",
    "\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            maximum_ages.append(maximum_age.text.strip())\n",
    "\n",
    "        if(i>500):\n",
    "            break\n",
    "tar.close()\n",
    "\n",
    "\n",
    "if(len(doc_ids) == 0):\n",
    "    print(\"doc_ids\")\n",
    "if(len(brief_titles) == 0):\n",
    "    print(\"brief_titles\")\n",
    "if(len(detailed_descriptions) == 0):\n",
    "    print(\"detailed_descriptions\")\n",
    "if(len(brief_summaries) == 0):\n",
    "    print(\"brief_summaries\")\n",
    "if(len(criterias) == 0):\n",
    "    print(\"criterias\")\n",
    "if(len(genders) == 0):\n",
    "    print(\"genders\")\n",
    "if(len(minimum_ages) == 0):\n",
    "    print(\"minimum_ages\")\n",
    "if(len(maximum_ages) == 0):\n",
    "    print(\"maximum_ages\")\n",
    "#Aqui criamos os docs pickle para cada uma das partes dos documentos\n",
    "pickle.dump(doc_ids, open(\"doc_ids.bin\", \"wb\" ))\n",
    "pickle.dump(brief_titles, open(\"brief_title.bin\", \"wb\" ))\n",
    "pickle.dump(detailed_descriptions, open(\"detailed_description.bin\", \"wb\" ))\n",
    "pickle.dump(brief_summaries, open(\"brief_summary.bin\", \"wb\" ))\n",
    "pickle.dump(criterias, open(\"criteria.bin\", \"wb\" ))\n",
    "pickle.dump(genders, open(\"gender.bin\", \"wb\" ))\n",
    "pickle.dump(minimum_ages, open(\"minimum_age.bin\", \"wb\" ))\n",
    "pickle.dump(maximum_ages, open(\"maximum_age.bin\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classe RetrievalModel: definimos a classe abstrata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class RetrievalModel:\n",
    "\n",
    "    def __init__(self, ids, docs):\n",
    "        self.ids = ids\n",
    "        self.docs = docs\n",
    "    \n",
    "    @abc.abstractmethod #para sabermos que RetrievalModel é uma classe abstrata e que, portanto, não pode ser instanciada, ie, \"concretizada\"\n",
    "    def search(self, cases):\n",
    "        pass #nao se pode por nada aqui na abstrata, apenas em cada classe \"filho\" é que se define a função \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class VSM(RetrievalModel): #aqui pomos o RetrievalModel para dizer q esta classe é uma subclasse da classe abstrata\n",
    "\n",
    "    def __init__(self, ids, docs):\n",
    "        super().__init__(ids, docs) #aqui dizemos que ela recebe os ids e docs que a superclasse recebe. sao os mesmos!\n",
    "\n",
    "\n",
    "    def search(self, cases): #aqui definimos a funcao que faz tudo o q o nosso modelo fazia \n",
    "        index = TfidfVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "        index.fit(self.docs)\n",
    "        X = index.transform(self.docs)\n",
    "\n",
    "        avg_precision_11point = np.zeros(11)\n",
    "        p10_list=[]\n",
    "        recall_list=[]\n",
    "        ap_list=[]\n",
    "        ndcg5_list=[]\n",
    "        mrr_list=[]\n",
    "            \n",
    "        for caseid in cases:\n",
    "            print(\"case id is\")\n",
    "            print(caseid)\n",
    "            query = cases[caseid]\n",
    "            query_tfidf = index.transform([query])\n",
    "            doc_scores = 1 - pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "            \n",
    "            results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "            print(\"dengue22 dengue\")\n",
    "            print(len(results))\n",
    "            results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "            print(\"dengue dengue\")\n",
    "            print(len(results_ord))\n",
    "            [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "            [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "            if (np.shape(recall_11point) != (0,)):\n",
    "                avg_precision_11point = avg_precision_11point + precision_11point\n",
    "\n",
    "            p10_list+=[p10]\n",
    "            recall_list+=[recall]\n",
    "            ap_list+=[ap]\n",
    "            ndcg5_list+=[ndcg5]\n",
    "            mrr_list+=[mrr]\n",
    "\n",
    "            return [np.mean(p10_list), np.mean(recall_list), np.mean(ap_list), np.mean(ndcg5_list), np.mean(mrr_list)]     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMJM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "class LMJM(RetrievalModel):\n",
    "\n",
    "    def __init__(self, ids, docs):\n",
    "        super().__init__(ids, docs)\n",
    "    \n",
    "    def search(self, cases):\n",
    "        index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "        X = index.fit(self.docs)\n",
    "        corpus_cv = index.transform(self.docs)\n",
    "\n",
    "        lmbd = 1\n",
    "\n",
    "        prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "        prob_word_corpus = np.sum(corpus_cv, axis=0)/np.sum(corpus_cv)  # p(t|mc)\n",
    "        log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "\n",
    "        for caseid in cases:\n",
    "            query = cases[caseid]\n",
    "            # print(query)\n",
    "            query_cv = index.transform([query])\n",
    "            # print(query_cv)\n",
    "\n",
    "            total = log_mixture*query_cv.T\n",
    "\n",
    "            results = pd.DataFrame(list(zip(ids, total)), columns=['_id', 'score'])\n",
    "            results_ord = results.sort_values(by=['score'], ascending=False)\n",
    "        \n",
    "\n",
    "            [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "            [precision_11point, recall_11point,\n",
    "                total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "            if (np.shape(recall_11point) != (0,)):\n",
    "                avg_precision_11point = avg_precision_11point + precision_11point\n",
    "                \n",
    "            #print(p10)\n",
    "            p10_list += [p10]\n",
    "            recall_list += [recall]\n",
    "            ap_list += [ap]\n",
    "            ndcg5_list += [ndcg5]\n",
    "            mrr_list += [mrr]\n",
    "\n",
    "        return [np.mean(p10_list), np.mean(recall_list), np.mean(ap_list), np.mean(ndcg5_list), np.mean(mrr_list)]     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamar as classes para obtermos os valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases is\n",
      "60\n",
      "case id is\n",
      "20141\n",
      "dengue22 dengue\n",
      "0\n",
      "dengue dengue\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6036/3414740619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mmodels2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mVSM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpart\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLMJM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpart\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6036/1131542199.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, cases)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dengue dengue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_ord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;33m[\u001b[0m\u001b[0mp10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndcg5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_ord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaseid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[0mprecision_11point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_11point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_relv_ret\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevalPR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_ord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaseid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Ambiente de Trabalho\\FCT\\RI\\Projeto\\InformationRetrieval\\trec.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, result, query_id)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Normalized Discount Cummulative Gain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mp10\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelev_judg_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mndcg5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndcg_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelev_judg_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelev_judg_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_relevant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "#Aqui abrimos cada documento pickle \n",
    "doc_id= pickle.load(open(\"doc_ids.bin\", \"rb\" ))\n",
    "brief_title = pickle.load(open(\"brief_title.bin\", \"rb\" ))\n",
    "detailed_description = pickle.load(open(\"detailed_description.bin\", \"rb\" ))\n",
    "brief_summary = pickle.load(open(\"brief_summary.bin\", \"rb\" ))\n",
    "criteria = pickle.load(open(\"criteria.bin\", \"rb\" ))\n",
    "gender = pickle.load(open(\"gender.bin\", \"rb\" ))\n",
    "minimum_age = pickle.load(open(\"minimum_age.bin\", \"rb\" ))\n",
    "maximum_age = pickle.load(open(\"maximum_age.bin\", \"rb\" ))\n",
    "cases =  pickle.load(open(\"cases.bin\", \"rb\" ))\n",
    "print(\"cases is\")\n",
    "print(len(cases))\n",
    "corpus_parts = [brief_title, detailed_description, brief_summary, criteria]\n",
    "\n",
    "\n",
    "models = [\"VSM_brief_title\", \"VSM_detailed_description\", \"VSM_brief_summary\", \"VSM_criteria\", \n",
    "\"LMJM_brief_title\", \"LMJM_detailed_description\", \"LMJM_brief_summary\", \"LMJM_criteria\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for part in corpus_parts:\n",
    "    #print(part)\n",
    "    models2 = [VSM(doc_id, part ), LMJM(doc_id, part )]\n",
    "    for model in models2:\n",
    "        results += model.search(cases)\n",
    "    \n",
    "for i in range(len(models)):\n",
    "    print(models[i] + \" : \" + results[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: average-prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recall_11point' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6036/3530622827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_11point\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_precision_11point\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'recall_11point' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recall_11point,avg_precision_11point/len(cases))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
