{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Setting groundtruth and queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "\n",
    "\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "cases_age = {}\n",
    "cases_genre = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "    cases_age[q_num] = query.find('AGE').text\n",
    "    cases_genre[q_num] = query.find('GENDER').text\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)\n",
    "\n",
    "pickle.dump(cases, open(\"cases.bin\", \"wb\"))\n",
    "pickle.dump(cases_age, open(\"cases_age.bin\", \"wb\"))\n",
    "pickle.dump(cases_genre, open(\"cases_genre.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 -Getting Clinical trials info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids 3626\n",
      "bt 3626\n",
      "dd 3626\n",
      "bs 3626\n",
      "c 3626\n",
      "g 3626\n",
      "mina 3626\n",
      "maa 3626\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "doc_ids = []\n",
    "brief_titles = []\n",
    "detailed_descriptions = []\n",
    "brief_summaries = []\n",
    "criterias = []\n",
    "genders = []\n",
    "minimum_ages = []\n",
    "maximum_ages = []\n",
    "\n",
    "namelist = tar.getnames()\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "        judged = False\n",
    "        id=''\n",
    "        bt=''\n",
    "        dt=''\n",
    "        bs=''\n",
    "        cs=''\n",
    "        ge=''\n",
    "        ma=''\n",
    "        mi=''\n",
    "\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "                id=doc_id.text.strip()\n",
    "                break\n",
    "                \n",
    "        if judged is False:\n",
    "            continue\n",
    "\n",
    "        brief_title = root.find('brief_title').text.strip()\n",
    "        bt=brief_title\n",
    "\n",
    "        for dd in root.iter('detailed_description'):\n",
    "            for child in dd:\n",
    "                dd=child.text.strip()\n",
    "        if dd == '':\n",
    "            dd=brief_title\n",
    "              \n",
    "        for bs in root.iter('brief_summary'):\n",
    "            for child in bs:\n",
    "                bs=child.text.strip()\n",
    "        if bs == '':\n",
    "            bs=brief_title\n",
    "                \n",
    "        for c in root.iter('criteria'):\n",
    "            for child in c:\n",
    "                cs=child.text.strip()\n",
    "        if cs=='':\n",
    "            cs= brief_title\n",
    "\n",
    "        for gender in root.iter('gender'):\n",
    "            ge=gender.text.strip()\n",
    "        if ge=='' or ge=='N/A':\n",
    "            ge='Both'\n",
    "\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            mi=minimum_age.text.strip()\n",
    "        if mi=='' or mi=='N/A':\n",
    "            mi='0 years'\n",
    "\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            ma=maximum_age.text.strip()\n",
    "        if ma=='' or ma=='N/A':\n",
    "            ma='100 years'\n",
    "\n",
    "       \n",
    "        if id!='' and bt!='' and dd!='' and bs!='' and cs!='' and ge!='' and mi!='' and ma!='':\n",
    "            doc_ids.append(id)\n",
    "            brief_titles.append(bt)\n",
    "            detailed_descriptions.append(dd)\n",
    "            brief_summaries.append(bs)\n",
    "            criterias.append(cs)\n",
    "            genders.append(ge)\n",
    "            minimum_ages.append(mi)\n",
    "            maximum_ages.append(ma)\n",
    "            #print('passed')\n",
    "        else:\n",
    "            print('failed')\n",
    "            print(id,bt,dd,bs,cs)\n",
    "            continue\n",
    "\n",
    "        if len(doc_ids) != len(brief_titles) or len(detailed_descriptions) != len(brief_summaries) or len(brief_summaries) != len(criterias):\n",
    "            print(\"error\")\n",
    "            print(brief_title)\n",
    "            print(detailed_descriptions[0].text.strip())\n",
    "            print(brief_summaries[0].text.strip())\n",
    "            print(criterias[0].text.strip())\n",
    "            break\n",
    "tar.close()\n",
    "\n",
    "print('ids',len(doc_ids))\n",
    "print('bt', len(brief_titles))\n",
    "print('dd',len(detailed_descriptions))\n",
    "print('bs',len(brief_summaries))\n",
    "print('c',len(criterias))\n",
    "print('g', len(genders))\n",
    "print('mina',len(minimum_ages))\n",
    "print('maa',len(maximum_ages))\n",
    "\n",
    "pickle.dump(doc_ids, open(\"doc_ids.bin\", \"wb\"))\n",
    "pickle.dump(brief_titles, open(\"brief_title.bin\", \"wb\"))\n",
    "pickle.dump(detailed_descriptions, open(\"detailed_description.bin\", \"wb\"))\n",
    "pickle.dump(brief_summaries, open(\"brief_summary.bin\", \"wb\"))\n",
    "pickle.dump(criterias, open(\"criteria.bin\", \"wb\"))\n",
    "pickle.dump(genders, open(\"gender.bin\", \"wb\"))\n",
    "pickle.dump(minimum_ages, open(\"minimum_age.bin\", \"wb\"))\n",
    "pickle.dump(maximum_ages, open(\"maximum_age.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 -Defining abstract class RetrievalModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc  # é preciso importar isto quando queremos definir uma classe abstrata\n",
    "\n",
    "class RetrievalModel:  # vamos criar uma classe abstrata que é o molde para todas as nossas classes, cada uma um modelo\n",
    "    @abc.abstractmethod  # para sabermos que RetrievalModel é uma classe abstrata e que, portanto, não pode ser instanciada, ie, \"concretizada\"\n",
    "    def search(self):  # aqui nomeia-se uma das funcoes desta classe, neste caso, aquela onde vamos por o codigo q ordenava os docs e ainda classificava a performance do nosso modelo (junto para nao termos q mudar tanto o codigo)\n",
    "        pass  # nao se pode por nada aqui na abstrata, apenas em cada classe \"filho\" é que se define a função, aqui apenas se nomeia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-VSM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class VSM(RetrievalModel):  # definimos a classe de um dos modelos e pomos o RetrievalModel para dizer q esta classe é uma subclasse da classe abstrata\n",
    "    def search(self, caseid, docs):  # aqui definimos a funcao que faz tudo o q o nosso modelo fazia, pus o codigo ca dentro, pus self.doc em vez de docs\n",
    "        index = TfidfVectorizer(ngram_range=(\n",
    "            1, 1), analyzer='word', stop_words=None)\n",
    "        index.fit(docs)\n",
    "        X = index.transform(docs)\n",
    "        query = cases[caseid]\n",
    "        query_tfidf = index.transform([query])\n",
    "        doc_scores = 1-pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "        scores = doc_scores.tolist()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-LMJM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class LMJM(RetrievalModel):\n",
    "    def search(self, caseid, docs):\n",
    "        index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "        corpus_cv = index.fit(docs).transform(docs)\n",
    "        lmbd = 0.8\n",
    "        prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "        prob_word_corpus = np.sum(corpus_cv, axis=0) / \\\n",
    "            np.sum(corpus_cv)  # p(t|mc)\n",
    "        log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "        query = cases[caseid]\n",
    "        query_cv = index.transform([query])\n",
    "        total = log_mixture*query_cv.T\n",
    "        return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6-Import relevant documents and libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import trec\n",
    "import numpy as np\n",
    "\n",
    "# Aqui abrimos cada documento pickle e damos-lhes os nomes para usar nas funcoes seguintes\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\"))\n",
    "brief_title = pickle.load(open(\"brief_title.bin\", \"rb\"))\n",
    "detailed_description = pickle.load(open(\"detailed_description.bin\", \"rb\"))\n",
    "brief_summary = pickle.load(open(\"brief_summary.bin\", \"rb\"))\n",
    "criteria = pickle.load(open(\"criteria.bin\", \"rb\"))\n",
    "gender = pickle.load(open(\"gender.bin\", \"rb\"))\n",
    "minimum_age = pickle.load(open(\"minimum_age.bin\", \"rb\"))\n",
    "maximum_age = pickle.load(open(\"maximum_age.bin\", \"rb\"))\n",
    "cases = pickle.load(open(\"cases.bin\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-Setting models and text fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [VSM(), LMJM()]\n",
    "fields = [brief_title, detailed_description, brief_summary, criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-Separate training and test queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases length:  60\n",
      "training queries length:  48\n",
      "test queries length:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"cases length: \", len(cases))\n",
    "cases_training = []\n",
    "cases_test = []\n",
    "i = 0\n",
    "k = 12\n",
    "for caseid in cases:\n",
    "    if i < len(cases)-k:\n",
    "        cases_training.append(caseid)\n",
    "    else:\n",
    "        cases_test.append(caseid)\n",
    "    i += 1\n",
    "\n",
    "print(\"training queries length: \", len(cases_training))\n",
    "print(\"test queries length: \", len(cases_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-Getting scores for pairs query_doc for each model and text field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSM_brief_title_training:  3296\n",
      "VSM_detailed_description_training:  3296\n",
      "VSM_brief_summary_training:  3296\n",
      "VSM_criteria_training:  3296\n",
      "LMJM_brief_title_training:  3296\n",
      "LMJM_detailed_description_training:  3296\n",
      "LMJM_brief_summary_training:  3296\n",
      "LMJM_criteria_training:  3296\n",
      "y_training:  3296\n"
     ]
    }
   ],
   "source": [
    "# buscar listas\n",
    "queries_training = []\n",
    "docs_training = []\n",
    "VSM_bt_training = []\n",
    "VSM_dd_training = []\n",
    "VSM_bs_training = []\n",
    "VSM_cr_training = []\n",
    "LMJM_bt_training = []\n",
    "LMJM_dd_training = []\n",
    "LMJM_bs_training = []\n",
    "LMJM_cr_training = []\n",
    "y_training = []\n",
    "sample_weight=[]\n",
    "\n",
    "for caseid in cases_training:\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_training.append(rel)\n",
    "            sample_weight.append(0.0001)\n",
    "        elif rel == 1 :\n",
    "            y_training.append(1)\n",
    "            sample_weight.append(0.1)\n",
    "        elif rel == 2:\n",
    "            y_training.append(1)\n",
    "            sample_weight.append(0.5)\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel]\n",
    "                if field_ind == 0:\n",
    "                    queries_training.append(caseid)\n",
    "                    docs_training.append(rel)\n",
    "                    VSM_bt_training.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_training.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_training.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_training.append(value[0])\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_training.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_training.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_training.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_training.append(value[0])\n",
    "            field_ind += 1\n",
    "\n",
    "print(\"VSM_brief_title_training: \", len(VSM_bt_training))\n",
    "print(\"VSM_detailed_description_training: \", len(VSM_dd_training))\n",
    "print(\"VSM_brief_summary_training: \", len(VSM_bs_training))\n",
    "print(\"VSM_criteria_training: \", len(VSM_cr_training))\n",
    "print(\"LMJM_brief_title_training: \", len(LMJM_bt_training))\n",
    "print(\"LMJM_detailed_description_training: \", len(LMJM_dd_training))\n",
    "print(\"LMJM_brief_summary_training: \", len(LMJM_bs_training))\n",
    "print(\"LMJM_criteria_training: \", len(LMJM_cr_training))\n",
    "print(\"y_training: \", len(y_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LMJM_bt_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-Logistic Regression cross validation to find best C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        VSM_bt    VSM_dd    VSM_bs    VSM_cr                  LMJM_bt  \\\n",
      "2     0.104881  0.115946  0.099538  0.014328  [[[[[-116.37085398]]]]]   \n",
      "4     0.027550  0.024347  0.011509  0.032451  [[[[[-119.30575696]]]]]   \n",
      "6     0.011322  0.042630  0.014322  0.000000  [[[[[-120.47998841]]]]]   \n",
      "7     0.011181  0.035641  0.012324  0.000000  [[[[[-120.18397231]]]]]   \n",
      "10    0.044347  0.063335  0.028971  0.033145   [[[[[-115.1793773]]]]]   \n",
      "...        ...       ...       ...       ...                      ...   \n",
      "3280  0.081036  0.070022  0.051586  0.016380  [[[[[-152.93616104]]]]]   \n",
      "3284  0.030637  0.009683  0.071081  0.145103  [[[[[-154.04506576]]]]]   \n",
      "3287  0.000000  0.046847  0.057396  0.052143  [[[[[-159.76369396]]]]]   \n",
      "3289  0.007422  0.000000  0.010346  0.008244  [[[[[-157.53576359]]]]]   \n",
      "3294  0.000000  0.030083  0.008640  0.050659  [[[[[-159.76369396]]]]]   \n",
      "\n",
      "                      LMJM_dd                  LMJM_bs  \\\n",
      "2     [[[[[-138.00797573]]]]]  [[[[[-159.51411843]]]]]   \n",
      "4     [[[[[-154.37030609]]]]]  [[[[[-170.47507908]]]]]   \n",
      "6     [[[[[-147.49074071]]]]]  [[[[[-170.70335547]]]]]   \n",
      "7     [[[[[-145.89882075]]]]]  [[[[[-169.24260735]]]]]   \n",
      "10    [[[[[-146.86487958]]]]]  [[[[[-165.60750262]]]]]   \n",
      "...                       ...                      ...   \n",
      "3280  [[[[[-241.02643608]]]]]  [[[[[-241.33953983]]]]]   \n",
      "3284  [[[[[-253.39755779]]]]]  [[[[[-241.54009821]]]]]   \n",
      "3287    [[[[[-232.151609]]]]]  [[[[[-248.37833208]]]]]   \n",
      "3289  [[[[[-258.26249216]]]]]   [[[[[-254.1732283]]]]]   \n",
      "3294  [[[[[-241.96181504]]]]]  [[[[[-255.47264007]]]]]   \n",
      "\n",
      "                      LMJM_cr  \n",
      "2     [[[[[-168.26517279]]]]]  \n",
      "4      [[[[[-161.0059954]]]]]  \n",
      "6     [[[[[-173.39694171]]]]]  \n",
      "7     [[[[[-173.39694171]]]]]  \n",
      "10    [[[[[-163.54555642]]]]]  \n",
      "...                       ...  \n",
      "3280  [[[[[-254.42426948]]]]]  \n",
      "3284  [[[[[-237.65276739]]]]]  \n",
      "3287  [[[[[-247.27611198]]]]]  \n",
      "3289  [[[[[-256.99455085]]]]]  \n",
      "3294  [[[[[-242.08000046]]]]]  \n",
      "\n",
      "[660 rows x 8 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-414cac63844f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mpred_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mcand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdoc_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "candidates = {'query': queries_training,\n",
    "              'doc': docs_training,\n",
    "              'VSM_bt': VSM_bt_training,\n",
    "              'VSM_dd': VSM_dd_training,\n",
    "              'VSM_bs': VSM_bs_training,\n",
    "              'VSM_cr': VSM_cr_training,\n",
    "              'LMJM_bt': LMJM_bt_training,\n",
    "              'LMJM_dd': LMJM_dd_training,\n",
    "              'LMJM_bs': LMJM_bs_training,\n",
    "              'LMJM_cr': LMJM_cr_training,\n",
    "              'Y': y_training\n",
    "              }\n",
    "df = pd.DataFrame(candidates, columns=['query', 'doc',\n",
    "                                       'VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr', 'LMJM_bt', 'LMJM_dd', 'LMJM_bs', 'LMJM_cr', 'Y'])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = df[['VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr',\n",
    "        'LMJM_bt', 'LMJM_dd', 'LMJM_bs', 'LMJM_cr']]\n",
    "X_train = x.iloc[:, :]\n",
    "scaler.fit(X_train)\n",
    "#print(scaler.mean_)\n",
    "scaler.transform(X_train)\n",
    "#print(X_train)\n",
    "\n",
    "np.mean(X_train, axis=0)\n",
    "np.std(X_train, axis=0)\n",
    "\n",
    "y = df['Y']\n",
    "c_values = [0.01, 0.1, 1, 2, 3, 4, 5, 10]\n",
    "all_p10 = []\n",
    "all_recall = []\n",
    "all_ap = []\n",
    "all_ndcg5 = []\n",
    "all_mrr = []\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "\n",
    "for c in c_values:\n",
    "    p10_list=[]\n",
    "    recall_list=[]\n",
    "    ap_list=[]\n",
    "    ndcg5_list =[]\n",
    "    mrr_list =[]\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        pred_list = []\n",
    "        X_train, X_test = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = LogisticRegression(\n",
    "            random_state=0, C=c, class_weight='balanced').fit(X_train, y_train)\n",
    "            #sample_weight=sample_weight)\n",
    "        print(X_test)\n",
    "        for id in range(0, len(X_test)):\n",
    "            \n",
    "            pred= clf.coef_[0]*X_test[0] + clf.coef_[1]*X_test[id][1] + clf.coef_[2]*X_test[id][2] + clf.coef_[3]*X_test[id][3] + clf.coef_[4]*X_test[id][4] + clf.coef_[5]*X_test[id][5] + clf.coef_[6]*X_test[id][6] + clf.coef_[7]*X_test[id][7]\n",
    "            pred_list.append(pred)\n",
    "        cand = {'_id': doc_ids, 'score': pred_list}\n",
    "        results = pd.DataFrame(cand, columns=['_id', 'score'])\n",
    "        results.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "        [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "        p10_list += [p10]\n",
    "        recall_list += [recall]\n",
    "        ap_list += [ap]\n",
    "        ndcg5_list += [ndcg5]\n",
    "        mrr_list += [mrr]\n",
    "    \n",
    "\n",
    "    p10score = sum(p10_list)/k\n",
    "    recallscore = sum(recall_list)/k\n",
    "    apscore = sum(ap_list)/k\n",
    "    ndcg5score = sum(ndcg5_list)/k\n",
    "    mrrscore = sum(mrr_list)/k\n",
    "    all_p10 += [p10score]\n",
    "    all_recall += [recallscore]\n",
    "    all_ap += [apscore]\n",
    "    all_ndcg5 += [ndcg5score]\n",
    "    all_mrr += [mrrscore]\n",
    "\n",
    "plt.plot(c_values, all_p10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11-Training Logistic Regression with best C value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-73a49b375335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m clf = LogisticRegression(\n\u001b[0m\u001b[0;32m      2\u001b[0m     random_state=0, C=3, class_weight='balanced').fit(x, y)\n\u001b[0;32m      3\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Coefs: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    random_state=0, C=3, class_weight='balanced').fit(x, y)\n",
    "coefs = clf.coef_[0]\n",
    "print('Coefs: {}'.format(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12-Eval scores for docs that are judged for a given test query:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of relevant docs for test cases:\n",
      "P10 0.17500000000000002\n",
      "Recall 1.0\n",
      "AP 0.247741512981921\n",
      "NDCG5 0.12713573123234226\n",
      "MRR 0.2150800309473823\n"
     ]
    }
   ],
   "source": [
    "p10_list = []\n",
    "recall_list = []\n",
    "ap_list = []\n",
    "ndcg5_list = []\n",
    "mrr_list = []\n",
    "\n",
    "for caseid in cases_test:\n",
    "    VSM_bt_test = []\n",
    "    VSM_dd_test = []\n",
    "    VSM_bs_test = []\n",
    "    VSM_cr_test = []\n",
    "    LMJM_bt_test = []\n",
    "    LMJM_dd_test = []\n",
    "    LMJM_bs_test = []\n",
    "    LMJM_cr_test = []\n",
    "\n",
    "    y_test = []\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    zs = []\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_test.append(rel)\n",
    "        elif rel == 1 or rel == 2:\n",
    "            y_test.append(1)\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel]\n",
    "                if field_ind == 0:\n",
    "                    VSM_bt_test.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_test.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_test.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_test.append(value[0])\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_test.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_test.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_test.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_test.append(value[0])\n",
    "            field_ind += 1\n",
    "    for line in range(0, len(VSM_bt_test)):\n",
    "        z = coefs[0]*VSM_bt_test[line]+coefs[1]*VSM_dd_test[line] + coefs[2]*VSM_bs_test[line]+coefs[3]*VSM_cr_test[line] + \\\n",
    "            coefs[4]*LMJM_bt_test[line]+coefs[5]*LMJM_dd_test[line] + \\\n",
    "            coefs[6]*LMJM_bs_test[line]+coefs[7]*LMJM_cr_test[line]\n",
    "        zs.append(z)\n",
    "\n",
    "    doc_ids = []\n",
    "    for pos in case_rel:\n",
    "        doc_ids.append(ids[pos])\n",
    "\n",
    "    cand = {'_id': doc_ids, 'score': zs}\n",
    "    results = pd.DataFrame(cand, columns=['_id', 'score'])\n",
    "    results.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results, caseid)\n",
    "\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "print(\"Scores of relevant docs for test cases:\")\n",
    "print(\"P10\", np.mean(p10_list))\n",
    "print(\"Recall\", np.mean(recall_list))\n",
    "print(\"AP\", np.mean(ap_list))\n",
    "print(\"NDCG5\", np.mean(ndcg5_list))\n",
    "print(\"MRR\", np.mean(mrr_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13-Eval scores for all docs given test queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of all docs for test cases:\n",
      "P10 0.025000000000000005\n",
      "Recall 1.0\n",
      "AP 0.01004064180421022\n",
      "NDCG5 0.014131591792902345\n",
      "MRR 0.003125574554145983\n"
     ]
    }
   ],
   "source": [
    "p10_list = []\n",
    "recall_list = []\n",
    "ap_list = []\n",
    "ndcg5_list = []\n",
    "mrr_list = []\n",
    "\n",
    "avg_precision_11point = np.zeros(11)\n",
    "\n",
    "for caseid in cases_test:\n",
    "    VSM_bt_test = []\n",
    "    VSM_dd_test = []\n",
    "    VSM_bs_test = []\n",
    "    VSM_cr_test = []\n",
    "    LMJM_bt_test = []\n",
    "    LMJM_dd_test = []\n",
    "    LMJM_bs_test = []\n",
    "    LMJM_cr_test = []\n",
    "    field_ind = 0\n",
    "    zs = []\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for id in range(0, len(ids)):\n",
    "                value = scores[id]\n",
    "                if field_ind == 0:\n",
    "                    VSM_bt_test.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_test.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_test.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_test.append(value[0])\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_test.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_test.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_test.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_test.append(value[0])\n",
    "            field_ind += 1\n",
    "\n",
    "    for line in range(0, len(VSM_bt_test)):\n",
    "        z = coefs[0]*VSM_bt_test[line]+coefs[1]*VSM_dd_test[line] + coefs[2]*VSM_bs_test[line]+coefs[3]*VSM_cr_test[line] + \\\n",
    "            coefs[4]*LMJM_bt_test[line]+coefs[5]*LMJM_dd_test[line] + \\\n",
    "            coefs[6]*LMJM_bs_test[line]+coefs[7]*LMJM_cr_test[line]\n",
    "        zs.append(z)\n",
    "\n",
    "    cand = {'_id': ids, 'score': zs}\n",
    "    results = pd.DataFrame(cand, columns=['_id', 'score'])\n",
    "    results.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results, caseid)\n",
    "\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "print(\"Scores of all docs for test cases:\")\n",
    "print(\"P10\", np.mean(p10_list))\n",
    "print(\"Recall\", np.mean(recall_list))\n",
    "print(\"AP\", np.mean(ap_list))\n",
    "print(\"NDCG5\", np.mean(ndcg5_list))\n",
    "print(\"MRR\", np.mean(mrr_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14-Eval scores for all docs given test queries with filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_age = []\n",
    "d_min_age = []\n",
    "d_max_age = []\n",
    "\n",
    "\n",
    "for case in cases_age:\n",
    "    #print(case)\n",
    "    q_age.append(cases_age[case].split(' ')[0])\n",
    "\n",
    "for case in minimum_ages:\n",
    "    d_min_age.append(case.split(' ')[0])\n",
    "\n",
    "for case in maximum_ages:\n",
    "    d_max_age.append('65')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of all docs for test cases:\n",
      "P10 0.025000000000000005\n",
      "Recall 1.0\n",
      "AP 0.010041358553213238\n",
      "NDCG5 0.014131591792902345\n",
      "MRR 0.003126436781609195\n"
     ]
    }
   ],
   "source": [
    "p10_list = []\n",
    "recall_list = []\n",
    "ap_list = []\n",
    "ndcg5_list = []\n",
    "mrr_list = []\n",
    "\n",
    "avg_precision_11point = np.zeros(11)\n",
    "\n",
    "for caseid in cases_test:\n",
    "    new_ids = []\n",
    "    VSM_bt_test = []\n",
    "    VSM_dd_test = []\n",
    "    VSM_bs_test = []\n",
    "    VSM_cr_test = []\n",
    "    LMJM_bt_test = []\n",
    "    LMJM_dd_test = []\n",
    "    LMJM_bs_test = []\n",
    "    LMJM_cr_test = []\n",
    "    field_ind = 0\n",
    "    zs = []\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for id in range(0, len(ids)-1):\n",
    "                # filters\n",
    "                if q_age[0] <= d_max_age[id] and q_age[0] >= d_min_age[id]:\n",
    "                #and (cases_genre[caseid] == genders[id] or genders[id] == 'Both'):\n",
    "                    value = scores[id]\n",
    "                    if field_ind == 0:\n",
    "                        VSM_bt_test.append(value[0])\n",
    "                        new_ids.append(ids[id])\n",
    "                    elif field_ind == 1:\n",
    "                        VSM_dd_test.append(value[0])\n",
    "                    elif field_ind == 2:\n",
    "                        VSM_bs_test.append(value[0])\n",
    "                    elif field_ind == 3:\n",
    "                        VSM_cr_test.append(value[0])\n",
    "                    elif field_ind == 4:\n",
    "                        LMJM_bt_test.append(value[0])\n",
    "                    elif field_ind == 5:\n",
    "                        LMJM_dd_test.append(value[0])\n",
    "                    elif field_ind == 6:\n",
    "                        LMJM_bs_test.append(value[0])\n",
    "                    elif field_ind == 7:\n",
    "                        LMJM_cr_test.append(value[0])\n",
    "            field_ind += 1\n",
    "\n",
    "    for line in range(0, len(VSM_bt_test)):\n",
    "        z = coefs[0]*VSM_bt_test[line]+coefs[1]*VSM_dd_test[line] + coefs[2]*VSM_bs_test[line]+coefs[3]*VSM_cr_test[line] + \\\n",
    "            coefs[4]*LMJM_bt_test[line]+coefs[5]*LMJM_dd_test[line] + \\\n",
    "            coefs[6]*LMJM_bs_test[line]+coefs[7]*LMJM_cr_test[line]\n",
    "        zs.append(z)\n",
    "\n",
    "    cand = {'_id': new_ids, 'score': zs}\n",
    "    results = pd.DataFrame(cand, columns=['_id', 'score'])\n",
    "    results.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results, caseid)\n",
    "\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "print(\"Scores of all docs for test cases:\")\n",
    "print(\"P10\", np.mean(p10_list))\n",
    "print(\"Recall\", np.mean(recall_list))\n",
    "print(\"AP\", np.mean(ap_list))\n",
    "print(\"NDCG5\", np.mean(ndcg5_list))\n",
    "print(\"MRR\", np.mean(mrr_list))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
