{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "\n",
    "# https://wiki.python.org/moin/UsingPickle\n",
    "import pickle\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "i = 0\n",
    "ids = []\n",
    "docs = []\n",
    "\n",
    "iterations = 1000\n",
    "count = 0\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "\n",
    "        if judged is False:\n",
    "            continue\n",
    "\n",
    "        i = i + 1\n",
    "        \"\"\"\n",
    "        for brief_title in root.iter('brief_title'):\n",
    "            count += 1\n",
    "            docs.append(brief_title.text)\n",
    "            ids.append(doc_id.text)\n",
    "        continue\n",
    "        for detailed_description in root.iter('detailed_description'):\n",
    "            for child in detailed_description:\n",
    "                print(\"detailed_description: \", child.text.strip())\n",
    "        \"\"\"\n",
    "        for brief_summary in root.iter('brief_summary'):\n",
    "            for child in brief_summary:\n",
    "                docs.append(child.text.strip())\n",
    "                ids.append(doc_id.text)\n",
    "                #print(\"brief_summary: \", child.text.strip())\n",
    "\n",
    "        #if(i>10):\n",
    "            #break\n",
    "tar.close()\n",
    "\n",
    "pickle.dump(docs, open( \"documents.bin\", \"wb\" ) )\n",
    "pickle.dump(ids, open( \"doc_ids.bin\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = pickle.load( open( \"documents.bin\", \"rb\" ) )\n",
    "ids = pickle.load( open( \"doc_ids.bin\", \"rb\" ) )\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "index = TfidfVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "index.fit(docs)\n",
    "\n",
    "X = index.transform(docs)\n",
    "\n",
    "avg_precision_11point = np.zeros(11)\n",
    "\n",
    "p10_list=[]\n",
    "recall_list=[]\n",
    "ap_list=[]\n",
    "ndcg5_list=[]\n",
    "mrr_list=[]\n",
    "\n",
    "for caseid in cases:\n",
    "    query = cases[caseid]\n",
    "    query_tfidf = index.transform([query])\n",
    "    doc_scores = 1 - pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "    \n",
    "    results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "    results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "    \n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "    [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "    if (np.shape(recall_11point) != (0,)):\n",
    "        avg_precision_11point = avg_precision_11point + precision_11point\n",
    "\n",
    "    p10_list+=[p10]\n",
    "    recall_list+=[recall]\n",
    "    ap_list+=[ap]\n",
    "    ndcg5_list+=[ndcg5]\n",
    "    mrr_list+=[mrr]\n",
    "\n",
    "\"\"\"  \n",
    "print(np.mean(p10_list))  \n",
    "print(np.mean(recall_list)) \n",
    "print(np.mean(ap_list)) \n",
    "print(np.mean(ndcg5_list)) \n",
    "print(np.mean(mrr_list)) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = pickle.load( open( \"documents.bin\", \"rb\" ) )\n",
    "ids = pickle.load( open( \"doc_ids.bin\", \"rb\" ) )\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "index = TfidfVectorizer(ngram_range=(2,2), analyzer='word', stop_words = None)\n",
    "index.fit(docs)\n",
    "\n",
    "X = index.transform(docs)\n",
    "\n",
    "avg_precision_11point = np.zeros(11)\n",
    "\n",
    "p10_list=[]\n",
    "recall_list=[]\n",
    "ap_list=[]\n",
    "ndcg5_list=[]\n",
    "mrr_list=[]\n",
    "\n",
    "for caseid in cases:\n",
    "    query = cases[caseid]\n",
    "    query_tfidf = index.transform([query])\n",
    "    doc_scores = 1 - pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "    \n",
    "    results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "    results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "    \n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "    [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "    if (np.shape(recall_11point) != (0,)):\n",
    "        avg_precision_11point = avg_precision_11point + precision_11point\n",
    "\n",
    "    p10_list+=[p10]\n",
    "    recall_list+=[recall]\n",
    "    ap_list+=[ap]\n",
    "    ndcg5_list+=[ndcg5]\n",
    "    mrr_list+=[mrr]\n",
    "\n",
    "\"\"\"\n",
    "print(np.mean(p10_list))  \n",
    "print(np.mean(recall_list)) \n",
    "print(np.mean(ap_list)) \n",
    "print(np.mean(ndcg5_list)) \n",
    "print(np.mean(mrr_list)) \n",
    "\"\"\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMJM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "docs = pickle.load(open(\"documents.bin\", \"rb\"))\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\"))\n",
    "\n",
    "\n",
    "index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "X = index.fit(docs)\n",
    "\n",
    "corpus_cv = index.transform(docs)\n",
    "\n",
    "\n",
    "lmbd = 1\n",
    "\n",
    "prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "prob_word_corpus = np.sum(corpus_cv, axis=0)/np.sum(corpus_cv)  # p(t|mc)\n",
    "log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "\n",
    "\n",
    "for caseid in cases:\n",
    "    query = cases[caseid]\n",
    "    # print(query)\n",
    "    query_cv = index.transform([query])\n",
    "    # print(query_cv)\n",
    "\n",
    "    total = log_mixture*query_cv.T\n",
    "\n",
    "    results = pd.DataFrame(list(zip(ids, total)), columns=['_id', 'score'])\n",
    "    results_ord = results.sort_values(by=['score'], ascending=False)\n",
    "   \n",
    "\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "    if (np.shape(recall_11point) != (0,)):\n",
    "        avg_precision_11point = avg_precision_11point + precision_11point\n",
    "        \n",
    "    #print(p10)\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "\"\"\"\n",
    "print(np.mean(p10_list))\n",
    "print(np.mean(recall_list))\n",
    "print(np.mean(ap_list))\n",
    "print(np.mean(ndcg5_list))\n",
    "print(np.mean(mrr_list))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "docs = pickle.load(open(\"documents.bin\", \"rb\"))\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\"))\n",
    "\n",
    "\n",
    "index = CountVectorizer(ngram_range=(2, 2), analyzer='word')\n",
    "X = index.fit(docs)\n",
    "\n",
    "corpus_cv = index.transform(docs)\n",
    "\n",
    "lmbd = 1\n",
    "\n",
    "prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "prob_word_corpus = np.sum(corpus_cv, axis=0)/np.sum(corpus_cv)  # p(t|mc)\n",
    "log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "\n",
    "for caseid in cases:\n",
    "    query = cases[caseid]\n",
    "    query_cv = index.transform([query])\n",
    "\n",
    "    total = log_mixture*query_cv.T\n",
    "\n",
    "    results = pd.DataFrame(list(zip(ids, total)), columns=['_id', 'score'])\n",
    "    results_ord = results.sort_values(by=['score'], ascending=False)\n",
    "    # print(results_ord)\n",
    "\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "    if (np.shape(recall_11point) != (0,)):\n",
    "        avg_precision_11point = avg_precision_11point + precision_11point\n",
    "    #print(p10)\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "\"\"\"\n",
    "print(np.mean(p10_list))\n",
    "print(np.mean(recall_list))\n",
    "print(np.mean(ap_list))\n",
    "print(np.mean(ndcg5_list))\n",
    "print(np.mean(mrr_list))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recall_11point,avg_precision_11point/len(cases))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
