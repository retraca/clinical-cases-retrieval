{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting groundtruth and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "\n",
    "\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)\n",
    "\n",
    "\n",
    "pickle.dump(cases, open(\"cases.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Clinical trials info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "doc_ids = []\n",
    "brief_titles = []\n",
    "detailed_descriptions = []\n",
    "brief_summaries = []\n",
    "criterias = []\n",
    "genders = []\n",
    "minimum_ages = []\n",
    "maximum_ages = []\n",
    "\n",
    "namelist=tar.getnames()\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        #print(txt)\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "                doc_ids.append(doc_id.text.strip())\n",
    "        if judged is False:\n",
    "            continue\n",
    "       \n",
    "        brief_title = root.find('brief_title').text.strip()\n",
    "        brief_titles.append(brief_title)\n",
    "     \n",
    "        try:\n",
    "            detailed_description = root.find('detailed_description').text.strip()\n",
    "            detailed_descriptions.append(detailed_description)\n",
    "        except:\n",
    "            detailed_descriptions.append(brief_title)\n",
    "\n",
    "        try:\n",
    "            brief_summary = root.find('brief_summary').text.strip()\n",
    "            brief_summaries.append(brief_summary)\n",
    "        except:\n",
    "            brief_summaries.append(brief_title)\n",
    "        \n",
    "        try:\n",
    "            criteria = root.find('criteria').text.strip\n",
    "            criterias.append(criteria)\n",
    "        except:\n",
    "            criterias.append(brief_title)\n",
    "\n",
    "\"\"\"\n",
    "        for gender in root.iter('gender'):\n",
    "            #print(gender)\n",
    "            genders.append(gender.text.strip())\n",
    "          \n",
    "\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            #print(minimum_age)\n",
    "            minimum_ages.append(minimum_age.text.strip())\n",
    "        \n",
    "\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            #print(maximum_age)\n",
    "            maximum_ages.append(maximum_age.text.strip())\n",
    "\"\"\"\n",
    "tar.close()\n",
    "\n",
    "print(\"brief title length: \", len(brief_titles))\n",
    "print(\"detailed description length: \", len(detailed_descriptions))\n",
    "print(\"brief summary length: \", len(brief_summaries))\n",
    "print(\"criteria length: \", len(criterias))\n",
    "print(\"genders length\", len(genders))\n",
    "print(\"minimum_age lenght\", len(minimum_ages))\n",
    "print(\"maximum_age lenght\", len(maximum_ages))\n",
    "\n",
    "\n",
    "# Aqui criamos os docs pickle para cada uma das partes dos documentos\n",
    "pickle.dump(doc_ids, open(\"doc_ids.bin\", \"wb\"))\n",
    "pickle.dump(brief_titles, open(\"brief_title.bin\", \"wb\"))\n",
    "pickle.dump(detailed_descriptions, open(\"detailed_description.bin\", \"wb\"))\n",
    "pickle.dump(brief_summaries, open(\"brief_summary.bin\", \"wb\"))\n",
    "pickle.dump(criterias, open(\"criteria.bin\", \"wb\"))\n",
    "pickle.dump(genders, open(\"gender.bin\", \"wb\"))\n",
    "pickle.dump(minimum_ages, open(\"minimum_age.bin\", \"wb\"))\n",
    "pickle.dump(maximum_ages, open(\"maximum_age.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining abstract class RetrievalModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc  # é preciso importar isto quando queremos definir uma classe abstrata\n",
    "\n",
    "\n",
    "class RetrievalModel:  # vamos criar uma classe abstrata que é o molde para todas as nossas classes, cada uma um modelo\n",
    "    @abc.abstractmethod  # para sabermos que RetrievalModel é uma classe abstrata e que, portanto, não pode ser instanciada, ie, \"concretizada\"\n",
    "    def search(self):  # aqui nomeia-se uma das funcoes desta classe, neste caso, aquela onde vamos por o codigo q ordenava os docs e ainda classificava a performance do nosso modelo (junto para nao termos q mudar tanto o codigo)\n",
    "        pass  # nao se pode por nada aqui na abstrata, apenas em cada classe \"filho\" é que se define a função, aqui apenas se nomeia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class VSM(RetrievalModel):  # definimos a classe de um dos modelos e pomos o RetrievalModel para dizer q esta classe é uma subclasse da classe abstrata\n",
    "\n",
    "    def search(self, caseid, docs):  # aqui definimos a funcao que faz tudo o q o nosso modelo fazia, pus o codigo ca dentro, pus self.doc em vez de docs\n",
    "        index = TfidfVectorizer(ngram_range=(\n",
    "            1, 1), analyzer='word', stop_words=None)\n",
    "        index.fit(docs)\n",
    "        X = index.transform(docs)\n",
    "        query = cases[caseid]\n",
    "        query_tfidf = index.transform([query])\n",
    "        doc_scores = 1-pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "        scores = doc_scores.tolist()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMJM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class LMJM(RetrievalModel):\n",
    "    def search(self, caseid, docs):\n",
    "        index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "        X = index.fit(docs)\n",
    "        corpus_cv = index.transform(docs)\n",
    "        all_scores = []\n",
    "        lmbd = 1\n",
    "        prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "        prob_word_corpus = np.sum(corpus_cv, axis=0) / \\\n",
    "            np.sum(corpus_cv)  # p(t|mc)\n",
    "        log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "        query = cases[caseid]\n",
    "        query_cv = index.transform([query])\n",
    "        total = log_mixture*query_cv.T\n",
    "        return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant documents and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import trec\n",
    "import numpy as np\n",
    "\n",
    "# Aqui abrimos cada documento pickle e damos-lhes os nomes para usar nas funcoes seguintes\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\"))\n",
    "brief_title = pickle.load(open(\"brief_title.bin\", \"rb\"))\n",
    "detailed_description = pickle.load(open(\"detailed_description.bin\", \"rb\"))\n",
    "brief_summary = pickle.load(open(\"brief_summary.bin\", \"rb\"))\n",
    "criteria = pickle.load(open(\"criteria.bin\", \"rb\"))\n",
    "gender = pickle.load(open(\"gender.bin\", \"rb\"))\n",
    "minimum_age = pickle.load(open(\"minimum_age.bin\", \"rb\"))\n",
    "maximum_age = pickle.load(open(\"maximum_age.bin\", \"rb\"))\n",
    "cases = pickle.load(open(\"cases.bin\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting models and text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir modelos e campos\n",
    "models = [VSM()]\n",
    "#, LMJM()\n",
    "fields = [brief_title, detailed_description, brief_summary, criteria]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate training and test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases length:  60\n",
      "training queries length:  48\n",
      "test queries length:  12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"cases length: \", len(cases))\n",
    "cases_training = []\n",
    "cases_test = []\n",
    "i = 0\n",
    "k = 12\n",
    "for caseid in cases:\n",
    "    if i <= 11:\n",
    "        cases_test.append(caseid)\n",
    "    else:\n",
    "        cases_training.append(caseid)\n",
    "    i += 1\n",
    "\n",
    "print(\"training queries length: \", len(cases_training))\n",
    "print(\"test queries length: \", len(cases_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting scores for pairs query_doc for each model and text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920\n",
      "2920\n",
      "2920\n",
      "2920\n",
      "2920\n"
     ]
    }
   ],
   "source": [
    "# buscar listas\n",
    "queries_training = []\n",
    "docs_training = []\n",
    "VSM_bt_training = []\n",
    "VSM_dd_training = []\n",
    "VSM_bs_training = []\n",
    "VSM_cr_training = []\n",
    "LMJM_bt_training = []\n",
    "LMJM_dd_training = []\n",
    "LMJM_bs_training = []\n",
    "LMJM_cr_training = []\n",
    "y_training = []\n",
    "\n",
    "for caseid in cases_training:\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    #print(len(docs))\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_training.append(rel)\n",
    "        elif rel == 1 or rel == 2:\n",
    "            y_training.append(1)\n",
    "    #print(len(relevances))\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    #print(len(case_rel))\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel] \n",
    "                if field_ind == 0:\n",
    "                    queries_training.append(caseid)\n",
    "                    docs_training.append(rel)\n",
    "                    VSM_bt_training.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_training.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_training.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_training.append(value[0])\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_training.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_training.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_training.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_training.append(value[0])\n",
    "            field_ind += 1\n",
    "print(len(VSM_bt_training))\n",
    "print(len(VSM_dd_training))\n",
    "print(len(VSM_bs_training))\n",
    "print(len(VSM_cr_training))\n",
    "print(len(y_training))\n",
    "\n",
    "print(\"VSM_brief_title_training: \", len(VSM_bt_training))\n",
    "print(\"VSM_detailed_description_training: \", len(VSM_dd_training))\n",
    "print(\"VSM_brief_summary_training: \", len(VSM_bs_training))\n",
    "print(\"VSM_criteria_training: \", len(VSM_cr_training))\n",
    "\"\"\"\n",
    "print(\"LMJM_brief_title_training: \", len(LMJM_bt_training))\n",
    "print(\"LMJM_detailed_description_training: \", len(LMJM_dd_training))\n",
    "print(\"LMJM_brief_summary_training: \", len(LMJM_bs_training))\n",
    "print(\"LMJM_criteria_training: \", len(LMJM_cr_training))\n",
    "\"\"\"\n",
    "print(\"y_training: \", len(y_training))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression cross validation to find best C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular coeficientes da logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "candidates = {'query': queries_training,\n",
    "              'doc': docs_training,\n",
    "              'VSM_bt': VSM_bt_training,\n",
    "              'VSM_dd': VSM_dd_training,\n",
    "              'VSM_bs': VSM_bs_training,\n",
    "              'VSM_cr': VSM_cr_training,\n",
    "              'Y': y_training\n",
    "              }\n",
    "df = pd.DataFrame(candidates, columns=['query', 'doc',\n",
    "                                       'VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr', 'Y'])\n",
    "\n",
    "x = df[['VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr']]\n",
    "y = df['Y']\n",
    "\n",
    "c_values = [0.1, 0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2]\n",
    "c_ap = []\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "for c in c_values:\n",
    "    acc_score = []\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        X_train, X_test = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = LogisticRegression(\n",
    "            random_state=0, C=c, class_weight='balanced').fit(X_train, y_train)\n",
    "        pred_values = clf.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(pred_values, y_test)\n",
    "        acc_score.append(acc)\n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "    c_ap.append(avg_acc_score)\n",
    "\n",
    "plt.plot(c_values, c_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Logistic Regression with best C value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefs: [-0.33623517 -1.60532446 -0.29509851 -0.33623517]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(\n",
    "    random_state=0, C=0.3, class_weight='balanced').fit(x, y)\n",
    "coefs = clf.coef_[0]\n",
    "print('Coefs: {}'.format(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval scores for docs that are judged for a given test query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275\n",
      "1.0\n",
      "0.30758627981857345\n",
      "0.18429297351316887\n",
      "0.24984278055123996\n"
     ]
    }
   ],
   "source": [
    "p10_list=[]\n",
    "recall_list=[]\n",
    "ap_list=[]\n",
    "ndcg5_list=[]\n",
    "mrr_list=[]\n",
    "\n",
    "for caseid in cases_test:\n",
    "    VSM_bt_test = []\n",
    "    VSM_dd_test = []\n",
    "    VSM_bs_test = []\n",
    "    VSM_cr_test = []\n",
    "    \"\"\"\n",
    "    LMJM_bt_test = []\n",
    "    LMJM_dd_test = []\n",
    "    LMJM_bs_test = []\n",
    "    LMJM_cr_test = []\n",
    "    \"\"\"\n",
    "    y_test = []\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    zs = []\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_test.append(rel)\n",
    "        elif rel == 1 or rel == 2:\n",
    "            y_test.append(1)\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel]\n",
    "                if field_ind == 0:\n",
    "                    VSM_bt_test.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_test.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_test.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_test.append(value[0])\n",
    "                \"\"\"\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_test.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_test.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_test.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_test.append(value[0])\n",
    "                \"\"\"\n",
    "            field_ind += 1\n",
    "    for line in range(0, len(VSM_bt_test)):\n",
    "        z = coefs[0]*VSM_bt_test[line]+coefs[1]*VSM_dd_test[line] + coefs[2]*VSM_bs_test[line]+coefs[3]*VSM_cr_test[line]\n",
    "        zs.append(z)\n",
    "\n",
    "    doc_ids=[]\n",
    "    for pos in case_rel:\n",
    "        doc_ids.append(ids[pos])\n",
    "\n",
    "    cand={'_id': doc_ids, 'score': zs}\n",
    "    results = pd.DataFrame(cand, columns = ['_id', 'score'])\n",
    "    results.sort_values(by=['Z'], inplace=True, ascending=True)\n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "\n",
    "print(\"P10\", np.mean(p10_list))\n",
    "print(\"Recall\", np.mean(recall_list))\n",
    "print(\"AP\", np.mean(ap_list))\n",
    "print(\"NDCG5\", np.mean(ndcg5_list))\n",
    "print(\"MRR\", np.mean(mrr_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval scores for all docs given test queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
