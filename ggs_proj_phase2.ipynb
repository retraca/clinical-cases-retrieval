{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)\n",
    "pickle.dump(cases, open(\"cases.bin\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "i = 0\n",
    "\n",
    "doc_ids = []\n",
    "brief_titles = []\n",
    "detailed_descriptions = []\n",
    "brief_summaries = []\n",
    "criterias = []\n",
    "genders = []\n",
    "minimum_ages = []\n",
    "maximum_ages = []\n",
    "\n",
    "\n",
    "iterations = 1000\n",
    "count = 0\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "                doc_ids.append(doc_id.text.strip())\n",
    "        \n",
    "        if judged is False:\n",
    "            continue\n",
    "        i = i + 1\n",
    "      \n",
    "        for brief_title in root.iter('brief_title'):\n",
    "            brief_titles.append(brief_title.text.strip()) #para os brief titles nao se usa o child, o texto está direto apos <brief_title>\n",
    "\n",
    "        for detailed_description in root.iter('detailed_description'):\n",
    "            for child in detailed_description:\n",
    "                detailed_descriptions.append(child.text.strip()) #aqui, dentro do append temos que usar o child pq, se virem no documento dos clinical tirals, o texto detailed description esta dentro de um novo separadorzinho\n",
    "\n",
    "        for brief_summary in root.iter('brief_summary'):\n",
    "            for child in brief_summary:\n",
    "                brief_summaries.append(child.text.strip())\n",
    "\n",
    "        for criteria in root.iter('criteria'):\n",
    "            for child in criteria:\n",
    "                criterias.append(child.text.strip())\n",
    "\n",
    "        for gender in root.iter('gender'):\n",
    "            genders.append(gender.text.strip())\n",
    "\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            minimum_ages.append(minimum_age.text.strip())\n",
    "\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            maximum_ages.append(maximum_age.text.strip())\n",
    "\n",
    "        if(i>1000):\n",
    "            break\n",
    "tar.close()\n",
    "\n",
    "\n",
    "#Aqui criamos os docs pickle para cada uma das partes dos documentos\n",
    "pickle.dump(doc_ids, open(\"doc_ids.bin\", \"wb\" ))\n",
    "pickle.dump(brief_titles, open(\"brief_title.bin\", \"wb\" ))\n",
    "pickle.dump(detailed_descriptions, open(\"detailed_description.bin\", \"wb\" ))\n",
    "pickle.dump(brief_summaries, open(\"brief_summary.bin\", \"wb\" ))\n",
    "pickle.dump(criterias, open(\"criteria.bin\", \"wb\" ))\n",
    "pickle.dump(genders, open(\"gender.bin\", \"wb\" ))\n",
    "pickle.dump(minimum_ages, open(\"minimum_age.bin\", \"wb\" ))\n",
    "pickle.dump(maximum_ages, open(\"maximum_age.bin\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classe RetrievalModel: definimos a classe abstrata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc #é preciso importar isto quando queremos definir uma classe abstrata\n",
    "\n",
    "class RetrievalModel: #vamos criar uma classe abstrata que é o molde para todas as nossas classes, cada uma um modelo\n",
    "\n",
    "    def __init__(self, ids, docs): #aqui definimos as variaveis que entram na classe, sempre q as queremos usar temos que chamar por self.nome_da_variavel, exceto dentro do init das subclasses\n",
    "        self.ids = ids\n",
    "        self.docs = docs\n",
    "    \n",
    "    @abc.abstractmethod #para sabermos que RetrievalModel é uma classe abstrata e que, portanto, não pode ser instanciada, ie, \"concretizada\"\n",
    "    def search(self, cases): #aqui nomeia-se uma das funcoes desta classe, neste caso, aquela onde vamos por o codigo q ordenava os docs e ainda classificava a performance do nosso modelo (junto para nao termos q mudar tanto o codigo)\n",
    "        pass #nao se pode por nada aqui na abstrata, apenas em cada classe \"filho\" é que se define a função, aqui apenas se nomeia \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class VSM(RetrievalModel): #definimos a classe de um dos modelos e pomos o RetrievalModel para dizer q esta classe é uma subclasse da classe abstrata\n",
    "\n",
    "    def __init__(self, ids, docs):\n",
    "        super().__init__(ids, docs) #aqui dizemos que ela recebe os ids e docs que a superclasse recebe. sao os mesmos!\n",
    "\n",
    "\n",
    "    def search(self, cases): #aqui definimos a funcao que faz tudo o q o nosso modelo fazia, pus o codigo ca dentro, pus self.doc em vez de docs \n",
    "        index = TfidfVectorizer(ngram_range=(1,1), analyzer='word', stop_words = None)\n",
    "        index.fit(self.docs)\n",
    "        X = index.transform(self.docs)\n",
    "\n",
    "        avg_precision_11point = np.zeros(11)\n",
    "        p10_list=[]\n",
    "        recall_list=[]\n",
    "        ap_list=[]\n",
    "        ndcg5_list=[]\n",
    "        mrr_list=[]\n",
    "            \n",
    "        for caseid in cases:\n",
    "            query = cases[caseid]\n",
    "            query_tfidf = index.transform([query])\n",
    "            doc_scores = 1 - pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "            \n",
    "            results = pd.DataFrame(list(zip(ids, doc_scores)), columns = ['_id', 'score'])\n",
    "            results_ord = results.sort_values(by=['score'], ascending = False)\n",
    "\n",
    "            [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "            [precision_11point, recall_11point, total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "            if (np.shape(recall_11point) != (0,)):\n",
    "                avg_precision_11point = avg_precision_11point + precision_11point\n",
    "\n",
    "            p10_list+=[p10]\n",
    "            recall_list+=[recall]\n",
    "            ap_list+=[ap]\n",
    "            ndcg5_list+=[ndcg5]\n",
    "            mrr_list+=[mrr]\n",
    "\n",
    "            return [[np.mean(p10_list), np.mean(recall_list), np.mean(ap_list), np.mean(ndcg5_list), np.mean(mrr_list), recall_11point, avg_precision_11point]]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMJM Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "class LMJM(RetrievalModel):\n",
    "\n",
    "    def __init__(self, ids, docs):\n",
    "        super().__init__(ids, docs)\n",
    "    \n",
    "    def search(self, cases):\n",
    "        index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "        X = index.fit(self.docs)\n",
    "        corpus_cv = index.transform(self.docs)\n",
    "\n",
    "        avg_precision_11point = np.zeros(11)\n",
    "        p10_list=[]\n",
    "        recall_list=[]\n",
    "        ap_list=[]\n",
    "        ndcg5_list=[]\n",
    "        mrr_list=[]\n",
    "        \n",
    "        lmbd = 1\n",
    "\n",
    "        prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "        prob_word_corpus = np.sum(corpus_cv, axis=0)/np.sum(corpus_cv)  # p(t|mc)\n",
    "        log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "\n",
    "        for caseid in cases:\n",
    "            query = cases[caseid]\n",
    "            query_cv = index.transform([query])\n",
    "\n",
    "            total = log_mixture*query_cv.T\n",
    "\n",
    "            results = pd.DataFrame(list(zip(ids, total)), columns=['_id', 'score'])\n",
    "            results_ord = results.sort_values(by=['score'], ascending=False)\n",
    "        \n",
    "            [p10, recall, ap, ndcg5, mrr] = eval.eval(results_ord, caseid)\n",
    "            [precision_11point, recall_11point,\n",
    "                total_relv_ret] = eval.evalPR(results_ord, caseid)\n",
    "\n",
    "            if (np.shape(recall_11point) != (0,)):\n",
    "                avg_precision_11point = avg_precision_11point + precision_11point\n",
    "                \n",
    "            #print(p10)\n",
    "            p10_list += [p10]\n",
    "            recall_list += [recall]\n",
    "            ap_list += [ap]\n",
    "            ndcg5_list += [ndcg5]\n",
    "            mrr_list += [mrr]\n",
    "\n",
    "        return [[np.mean(p10_list), np.mean(recall_list), np.mean(ap_list), np.mean(ndcg5_list), np.mean(mrr_list), recall_11point, avg_precision_11point]]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamar as classes para obtermos os valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp/ipykernel_4916/1306368024.py:28: RuntimeWarning: divide by zero encountered in log\n",
      "  log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSM_brief_title : [0.1, 0.34146341463414637, 0.011199555432409754, 0.0, 0.013986013986013986, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([0.16666667, 0.02287906, 0.0146081 , 0.01637321, 0.01409869,\n",
      "       0.01409869, 0.01409869, 0.01409869, 0.01409869, 0.01409869,\n",
      "       0.01409869])]\n",
      "VSM_detailed_description : [0.005000000000000001, 0.30075378943556946, 0.004364644387236058, 0.009469501675629268, 0.005827505827505828, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([2.25644084, 0.89071108, 0.54736405, 0.47969578, 0.46602912,\n",
      "       0.44271938, 0.41984128, 0.41992078, 0.42028755, 0.42088784,\n",
      "       0.4214063 ])]\n",
      "VSM_brief_summary : [0.0, 0.3170731707317073, 0.006911533491474804, 0.0, 0.016927083333333332, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([0.04      , 0.02180582, 0.0191582 , 0.01970144, 0.01706037,\n",
      "       0.01706037, 0.01706037, 0.01706037, 0.01706037, 0.01706037,\n",
      "       0.01706037])]\n",
      "VSM_criteria : [0.008333333333333333, 0.24901692612685686, 0.003648585223444014, 0.008479005131840404, 0.005946180555555556, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([2.33220436, 0.67259736, 0.60531206, 0.53637415, 0.49881606,\n",
      "       0.46634842, 0.46780496, 0.46789166, 0.46817723, 0.46874702,\n",
      "       0.46917192])]\n",
      "LMJM_brief_title : [0.1, 0.34146341463414637, 0.036630596629933315, 0.3391602052736161, 0.013986013986013986, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([1.        , 0.06240602, 0.01741795, 0.01844862, 0.01767677,\n",
      "       0.01767677, 0.01767677, 0.01767677, 0.01767677, 0.01767677,\n",
      "       0.01767677])]\n",
      "LMJM_detailed_description : [0.0033333333333333335, 0.30075378943556946, 0.003704658624041888, 0.005652670087893602, 0.005827505827505828, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([1.84923693, 0.58553505, 0.51752756, 0.47960328, 0.46593662,\n",
      "       0.44262689, 0.41974878, 0.41982828, 0.42019505, 0.42079534,\n",
      "       0.4213138 ])]\n",
      "LMJM_brief_summary : [0.3, 0.34146341463414637, 0.04135381733580009, 0.2530614822099702, 0.013986013986013986, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([0.5       , 0.03596226, 0.0159773 , 0.01272364, 0.01412714,\n",
      "       0.01412714, 0.01412714, 0.01412714, 0.01412714, 0.01412714,\n",
      "       0.01412714])]\n",
      "LMJM_criteria : [0.0033333333333333335, 0.30075378943556946, 0.003704658624041888, 0.005652670087893602, 0.005827505827505828, [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], array([1.84923693, 0.58553505, 0.51752756, 0.47960328, 0.46593662,\n",
      "       0.44262689, 0.41974878, 0.41982828, 0.42019505, 0.42079534,\n",
      "       0.4213138 ])]\n"
     ]
    }
   ],
   "source": [
    "#Aqui abrimos cada documento pickle e damos-lhes os nomes para usar nas funcoes seguintes\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\" ))\n",
    "brief_title = pickle.load(open(\"brief_title.bin\", \"rb\" ))\n",
    "detailed_description = pickle.load(open(\"detailed_description.bin\", \"rb\" ))\n",
    "brief_summary = pickle.load(open(\"brief_summary.bin\", \"rb\" ))\n",
    "criteria = pickle.load(open(\"criteria.bin\", \"rb\" ))\n",
    "gender = pickle.load(open(\"gender.bin\", \"rb\" ))\n",
    "minimum_age = pickle.load(open(\"minimum_age.bin\", \"rb\" ))\n",
    "maximum_age = pickle.load(open(\"maximum_age.bin\", \"rb\" ))\n",
    "cases =  pickle.load(open(\"cases.bin\", \"rb\" ))\n",
    "\n",
    "#Aqui definimos a lista de nomes que queremos dar a cada aplicacao do modelo. Pus nome do modelo + parte do corpus que usamos. S quiserem adicionar mais Classes que correspondam a modelos, nao se esquecam de as colocar aqui tambem!\n",
    "models = [\"VSM_brief_title\", \"VSM_detailed_description\", \"VSM_brief_summary\", \"VSM_criteria\", \n",
    "\"LMJM_brief_title\", \"LMJM_detailed_description\", \"LMJM_brief_summary\", \"LMJM_criteria\"]\n",
    "\n",
    "#Aqui fazemos uma lista com os nomes dos docs pickle que vamos usar como antes usavamos os docs\n",
    "corpus_parts = [brief_title, detailed_description, brief_summary, criteria]\n",
    "\n",
    "results = [] #Aqui criamos uma lista onde vao entrar listas que resultam do return de cada modelo (ver ultima linha da funcao search); portanto, sera uma lista A de listas B, em que cada lista B é o conjunto de resultados para a aplicacao do modelo correspondente \n",
    "\n",
    "for part in corpus_parts: #aqui dizemos que parte dos documentos dos clinical trials queremos usar \n",
    "    models2 = [VSM(doc_id, part ), LMJM(doc_id, part )] #definimos uma lista que inclui as duas classes (VSM e LMJM), em que ambas recebem o mesmo doc_id, mas part diferente dos clinical trials por cada vez que o \"for\" as chama. S quiserem adicionar mais Classes que correspondam a modelos, nao se esquecam de as colocar aqui tambem!\n",
    "    for model in models2: #aqui estamos a fazer os resultados para uma part do corpus do for anterior para cada modelo na lista models2.\n",
    "        results += model.search(cases)\n",
    "    \n",
    "for i in range(len(models)):\n",
    "    print(models[i] + \" : {}\".format(results[i])) #aqui imprimo o nome do modelo (que esta ordenado na lista models) seguido a lista de resultados; para ver qual numero corresponde a cada resultado, na classe que define o modelo, ver o return (ultima linha) da funcao search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot: average-prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recall_11point' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4916/3530622827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_11point\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_precision_11point\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'recall_11point' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recall_11point,avg_precision_11point/len(cases))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
