{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import trec\n",
    "import pprint as pp\n",
    "import pickle\n",
    "\n",
    "Qrels = \"qrels-clinical_trials.txt\"\n",
    "\n",
    "Queries = \"topics-2014_2015-summary.topics\"\n",
    "\n",
    "\n",
    "with open(Queries, 'r') as queries_reader:\n",
    "    txt = queries_reader.read()\n",
    "\n",
    "root = ET.fromstring(txt)\n",
    "\n",
    "cases = {}\n",
    "for query in root.iter('TOP'):\n",
    "    q_num = query.find('NUM').text\n",
    "    q_title = query.find('TITLE').text\n",
    "    cases[q_num] = q_title\n",
    "\n",
    "\n",
    "eval = trec.TrecEvaluation(cases, Qrels)\n",
    "\n",
    "\n",
    "pickle.dump(cases, open(\"cases.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(\"clinicaltrials.gov-16_dec_2015.tgz\", \"r:gz\")\n",
    "doc_ids = []\n",
    "brief_titles = []\n",
    "detailed_descriptions = []\n",
    "brief_summaries = []\n",
    "criterias = []\n",
    "genders = []\n",
    "minimum_ages = []\n",
    "maximum_ages = []\n",
    "#iterations = 1000\n",
    "#count = 0\n",
    "\n",
    "\n",
    "for tarinfo in tar:\n",
    "    if tarinfo.size > 500:\n",
    "        txt = tar.extractfile(tarinfo).read().decode(\"utf-8\", \"strict\")\n",
    "        root = ET.fromstring(txt)\n",
    "\n",
    "        judged = False\n",
    "        for doc_id in root.iter('nct_id'):\n",
    "            if doc_id.text in eval.judged_docs:\n",
    "                judged = True\n",
    "                doc_ids.append(doc_id.text.strip())\n",
    "\n",
    "        if judged is False:\n",
    "            continue\n",
    "\n",
    "        for brief_title in root.iter('brief_title'):\n",
    "            # para os brief titles nao se usa o child, o texto está direto apos <brief_title>\n",
    "            brief_titles.append(brief_title.text.strip())\n",
    "\n",
    "        for detailed_description in root.iter('detailed_description'):\n",
    "            for child in detailed_description:\n",
    "                # aqui, dentro do append temos que usar o child pq, se virem no documento dos clinical tirals, o texto detailed description esta dentro de um novo separadorzinho\n",
    "                detailed_descriptions.append(child.text.strip())\n",
    "\n",
    "        for brief_summary in root.iter('brief_summary'):\n",
    "            for child in brief_summary:\n",
    "                brief_summaries.append(child.text.strip())\n",
    "\n",
    "        for criteria in root.iter('criteria'):\n",
    "            for child in criteria:\n",
    "                criterias.append(child.text.strip())\n",
    "\n",
    "        for gender in root.iter('gender'):\n",
    "            genders.append(gender.text.strip())\n",
    "\n",
    "        for minimum_age in root.iter('minimum_age'):\n",
    "            minimum_ages.append(minimum_age.text.strip())\n",
    "\n",
    "        for maximum_age in root.iter('maximum_age'):\n",
    "            maximum_ages.append(maximum_age.text.strip())\n",
    "\n",
    "        # if(i>1000):\n",
    "            # break\n",
    "tar.close()\n",
    "\n",
    "\n",
    "# Aqui criamos os docs pickle para cada uma das partes dos documentos\n",
    "pickle.dump(doc_ids, open(\"doc_ids.bin\", \"wb\"))\n",
    "pickle.dump(brief_titles, open(\"brief_title.bin\", \"wb\"))\n",
    "pickle.dump(detailed_descriptions, open(\"detailed_description.bin\", \"wb\"))\n",
    "pickle.dump(brief_summaries, open(\"brief_summary.bin\", \"wb\"))\n",
    "pickle.dump(criterias, open(\"criteria.bin\", \"wb\"))\n",
    "pickle.dump(genders, open(\"gender.bin\", \"wb\"))\n",
    "pickle.dump(minimum_ages, open(\"minimum_age.bin\", \"wb\"))\n",
    "pickle.dump(maximum_ages, open(\"maximum_age.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classe RetrievalModel: definimos a classe abstrata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc  # é preciso importar isto quando queremos definir uma classe abstrata\n",
    "\n",
    "\n",
    "class RetrievalModel:  # vamos criar uma classe abstrata que é o molde para todas as nossas classes, cada uma um modelo\n",
    "    @abc.abstractmethod  # para sabermos que RetrievalModel é uma classe abstrata e que, portanto, não pode ser instanciada, ie, \"concretizada\"\n",
    "    def search(self):  # aqui nomeia-se uma das funcoes desta classe, neste caso, aquela onde vamos por o codigo q ordenava os docs e ainda classificava a performance do nosso modelo (junto para nao termos q mudar tanto o codigo)\n",
    "        pass  # nao se pode por nada aqui na abstrata, apenas em cada classe \"filho\" é que se define a função, aqui apenas se nomeia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VSM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class VSM(RetrievalModel):  # definimos a classe de um dos modelos e pomos o RetrievalModel para dizer q esta classe é uma subclasse da classe abstrata\n",
    "\n",
    "    def search(self, caseid, docs):  # aqui definimos a funcao que faz tudo o q o nosso modelo fazia, pus o codigo ca dentro, pus self.doc em vez de docs\n",
    "        index = TfidfVectorizer(ngram_range=(\n",
    "            1, 1), analyzer='word', stop_words=None)\n",
    "        index.fit(docs)\n",
    "        X = index.transform(docs)\n",
    "        query = cases[caseid]\n",
    "        query_tfidf = index.transform([query])\n",
    "        doc_scores = 1-pairwise_distances(X, query_tfidf, metric='cosine')\n",
    "        scores = doc_scores.tolist()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMJM Unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class LMJM(RetrievalModel):\n",
    "    def search(self, caseid, docs):\n",
    "        index = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n",
    "        X = index.fit(docs)\n",
    "        corpus_cv = index.transform(docs)\n",
    "        all_scores = []\n",
    "        lmbd = 1\n",
    "        prob_word_docs = corpus_cv/np.sum(corpus_cv, axis=1)  # p(t|md)\n",
    "        prob_word_corpus = np.sum(corpus_cv, axis=0) / \\\n",
    "            np.sum(corpus_cv)  # p(t|mc)\n",
    "        log_mixture = np.log(lmbd*prob_word_docs + (1-lmbd)*prob_word_corpus)\n",
    "        query = cases[caseid]\n",
    "        query_cv = index.transform([query])\n",
    "        total = log_mixture*query_cv.T\n",
    "        return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamar as classes para obtermos os valores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import trec\n",
    "import numpy as np\n",
    "\n",
    "# Aqui abrimos cada documento pickle e damos-lhes os nomes para usar nas funcoes seguintes\n",
    "ids = pickle.load(open(\"doc_ids.bin\", \"rb\"))\n",
    "brief_title = pickle.load(open(\"brief_title.bin\", \"rb\"))\n",
    "detailed_description = pickle.load(open(\"detailed_description.bin\", \"rb\"))\n",
    "brief_summary = pickle.load(open(\"brief_summary.bin\", \"rb\"))\n",
    "criteria = pickle.load(open(\"criteria.bin\", \"rb\"))\n",
    "gender = pickle.load(open(\"gender.bin\", \"rb\"))\n",
    "minimum_age = pickle.load(open(\"minimum_age.bin\", \"rb\"))\n",
    "maximum_age = pickle.load(open(\"maximum_age.bin\", \"rb\"))\n",
    "cases = pickle.load(open(\"cases.bin\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir modelos e campos\n",
    "models = [VSM()]\n",
    "#, LMJM()\n",
    "fields = [brief_title, detailed_description, brief_summary, criteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "48\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# separate training and test queries\n",
    "print(len(cases))\n",
    "cases_training = []\n",
    "cases_test = []\n",
    "i = 0\n",
    "k = 12\n",
    "for caseid in cases:\n",
    "    if i <= 11:\n",
    "        cases_test.append(caseid)\n",
    "    else:\n",
    "        cases_training.append(caseid)\n",
    "    i += 1\n",
    "print(len(cases_training))\n",
    "print(len(cases_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920\n",
      "2920\n",
      "2920\n",
      "2920\n",
      "2920\n"
     ]
    }
   ],
   "source": [
    "# buscar listas\n",
    "queries_training = []\n",
    "docs_training = []\n",
    "VSM_bt_training = []\n",
    "VSM_dd_training = []\n",
    "VSM_bs_training = []\n",
    "VSM_cr_training = []\n",
    "LMJM_bt_training = []\n",
    "LMJM_dd_training = []\n",
    "LMJM_bs_training = []\n",
    "LMJM_cr_training = []\n",
    "y_training = []\n",
    "\n",
    "for caseid in cases_training:\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    #print(len(docs))\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_training.append(rel)\n",
    "        elif rel == 1 or rel == 2:\n",
    "            y_training.append(1)\n",
    "    #print(len(relevances))\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    #print(len(case_rel))\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            if(field_ind == 0):\n",
    "                aux = scores\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel] if rel < len(scores) else aux[rel]\n",
    "                if field_ind == 0:\n",
    "                    queries_training.append(caseid)\n",
    "                    docs_training.append(rel)\n",
    "                    VSM_bt_training.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_training.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_training.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_training.append(value[0])\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_training.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_training.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_training.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_training.append(value[0])\n",
    "            field_ind += 1\n",
    "print(len(VSM_bt_training))\n",
    "print(len(VSM_dd_training))\n",
    "print(len(VSM_bs_training))\n",
    "print(len(VSM_cr_training))\n",
    "print(len(y_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       query   doc    VSM_bt    VSM_dd    VSM_bs    VSM_cr  Y\n",
      "0     201413   171  0.282372  0.012252  0.205578  0.224041  0\n",
      "1     201413   301  0.011587  0.038432  0.192411  0.147885  1\n",
      "2     201413   491  0.022157  0.009650  0.018708  0.003581  1\n",
      "3     201413   516  0.008460  0.024827  0.008676  0.035941  0\n",
      "4     201413   522  0.000000  0.011479  0.005058  0.028100  0\n",
      "...      ...   ...       ...       ...       ...       ... ..\n",
      "2915  201530  2946  0.000000  0.000000  0.017454  0.032015  0\n",
      "2916  201530  3130  0.000000  0.000000  0.013238  0.039843  0\n",
      "2917  201530  3208  0.025353  0.025353  0.033201  0.035362  0\n",
      "2918  201530  3363  0.121361  0.121361  0.017133  0.029767  0\n",
      "2919  201530  3382  0.024516  0.024516  0.035154  0.024513  0\n",
      "\n",
      "[2920 rows x 7 columns]\n",
      "[-0.24743389 -0.56546992 -0.92316441 -1.34035326]\n"
     ]
    }
   ],
   "source": [
    "#Calcular coeficientes da logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "candidates = {'query': queries_training,\n",
    "              'doc': docs_training,\n",
    "              'VSM_bt': VSM_bt_training,\n",
    "              'VSM_dd': VSM_dd_training,\n",
    "              'VSM_bs': VSM_bs_training,\n",
    "              'VSM_cr': VSM_cr_training,\n",
    "              'Y': y_training\n",
    "              }\n",
    "df = pd.DataFrame(candidates, columns=['query', 'doc',\n",
    "                                       'VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr', 'Y'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "x = df[['VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr']]\n",
    "y = df['Y']\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(x, y)\n",
    "coefs = clf.coef_[0]\n",
    "print(coefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCT00000408\n",
      "NCT00000492\n",
      "NCT00000501\n",
      "NCT00001853\n",
      "NCT00004727\n",
      "NCT00005127\n",
      "NCT00005485\n",
      "NCT00005757\n",
      "NCT00077948\n",
      "NCT00108381\n",
      "NCT00124969\n",
      "NCT00129233\n",
      "NCT00134160\n",
      "NCT00141583\n",
      "NCT00143195\n",
      "NCT00149227\n",
      "NCT00162344\n",
      "NCT00166231\n",
      "NCT00175279\n",
      "NCT00185120\n",
      "NCT00190489\n",
      "NCT00202566\n",
      "NCT00225355\n",
      "NCT00252421\n",
      "NCT00285649\n",
      "NCT00337116\n",
      "NCT00348803\n",
      "NCT00356707\n",
      "NCT00373828\n",
      "NCT00375089\n",
      "NCT00413712\n",
      "NCT00418392\n",
      "NCT00424021\n",
      "NCT00454662\n",
      "NCT00462241\n",
      "NCT00479908\n",
      "NCT00486954\n",
      "NCT00536224\n",
      "NCT00598728\n",
      "NCT00623454\n",
      "NCT00624273\n",
      "NCT00644319\n",
      "NCT00683813\n",
      "NCT00705952\n",
      "NCT00736333\n",
      "NCT00745043\n",
      "NCT00752674\n",
      "NCT00780260\n",
      "NCT00797953\n",
      "NCT00802230\n",
      "NCT00808652\n",
      "NCT00809029\n",
      "NCT00832442\n",
      "NCT00840593\n",
      "NCT00848250\n",
      "NCT00883987\n",
      "NCT00896285\n",
      "NCT00925119\n",
      "NCT00932867\n",
      "NCT00937365\n",
      "NCT00951808\n",
      "NCT00952744\n",
      "NCT00974116\n",
      "NCT00979199\n",
      "NCT00986180\n",
      "NCT00992927\n",
      "NCT01028534\n",
      "NCT01056367\n",
      "NCT01080274\n",
      "NCT01087723\n",
      "NCT01134328\n",
      "NCT01162902\n",
      "NCT01167205\n",
      "NCT01171911\n",
      "NCT01175889\n",
      "NCT01203696\n",
      "NCT01211171\n",
      "NCT01216020\n",
      "NCT01228123\n",
      "NCT01231165\n",
      "NCT01250262\n",
      "NCT01253486\n",
      "NCT01276548\n",
      "NCT01284179\n",
      "NCT01290770\n",
      "NCT01293019\n",
      "NCT01332188\n",
      "NCT01334203\n",
      "NCT01365286\n",
      "NCT01397994\n",
      "NCT01407146\n",
      "NCT01429194\n",
      "NCT01444833\n",
      "NCT01450020\n",
      "NCT01484912\n",
      "NCT01494870\n",
      "NCT01506999\n",
      "NCT01559948\n",
      "NCT01561001\n",
      "NCT01576978\n",
      "NCT01657136\n",
      "NCT01658033\n",
      "NCT01660594\n",
      "NCT01675362\n",
      "NCT01682057\n",
      "NCT01719952\n",
      "NCT01724567\n",
      "NCT01724996\n",
      "NCT01736306\n",
      "NCT01770080\n",
      "NCT01829659\n",
      "NCT01838616\n",
      "NCT01847014\n",
      "NCT01879293\n",
      "NCT01904695\n",
      "NCT02001545\n",
      "NCT02011646\n",
      "NCT02015858\n",
      "NCT02062424\n",
      "NCT02072694\n",
      "NCT02078921\n",
      "NCT02105220\n",
      "NCT02144636\n",
      "NCT02148289\n",
      "NCT02158962\n",
      "NCT02159625\n",
      "NCT02190123\n",
      "NCT02226510\n",
      "NCT02232607\n",
      "NCT02271568\n",
      "NCT02272920\n",
      "NCT02319161\n",
      "NCT02347111\n",
      "NCT02357212\n",
      "NCT02381340\n",
      "NCT02390323\n",
      "NCT02408263\n",
      "NCT02472938\n",
      "NCT02507050\n",
      "NCT02516839\n",
      "NCT02532699\n",
      "NCT02608255\n",
      "NCT02626741\n",
      "             _id     score\n",
      "0    NCT00000408 -0.615781\n",
      "1    NCT00000492 -0.054720\n",
      "2    NCT00000501 -0.201957\n",
      "3    NCT00001853 -0.145346\n",
      "4    NCT00004727 -0.093687\n",
      "..           ...       ...\n",
      "138  NCT02507050 -0.115881\n",
      "139  NCT02516839 -0.107565\n",
      "140  NCT02532699 -0.064604\n",
      "141  NCT02608255 -0.204825\n",
      "142  NCT02626741 -0.144113\n",
      "\n",
      "[143 rows x 2 columns]\n",
      "NCT00000932\n",
      "NCT00086879\n",
      "NCT00094887\n",
      "NCT00095316\n",
      "NCT00096772\n",
      "NCT00132951\n",
      "NCT00161772\n",
      "NCT00161798\n",
      "NCT00161824\n",
      "NCT00161863\n",
      "NCT00161967\n",
      "NCT00163540\n",
      "NCT00163618\n",
      "NCT00237016\n",
      "NCT00311441\n",
      "NCT00385632\n",
      "NCT00386035\n",
      "NCT00432003\n",
      "NCT00455468\n",
      "NCT00460486\n",
      "NCT00498654\n",
      "NCT00503529\n",
      "NCT00507871\n",
      "NCT00513500\n",
      "NCT00617344\n",
      "NCT00680836\n",
      "NCT00694655\n",
      "NCT00711399\n",
      "NCT00740155\n",
      "NCT00840801\n",
      "NCT00851487\n",
      "NCT00894686\n",
      "NCT01017081\n",
      "NCT01070732\n",
      "NCT01075204\n",
      "NCT01087892\n",
      "NCT01099943\n",
      "NCT01147445\n",
      "NCT01177657\n",
      "NCT01201252\n",
      "NCT01202201\n",
      "NCT01241201\n",
      "NCT01253967\n",
      "NCT01253980\n",
      "NCT01269554\n",
      "NCT01359020\n",
      "NCT01521403\n",
      "NCT01589926\n",
      "NCT01627327\n",
      "NCT01663155\n",
      "NCT01670149\n",
      "NCT01707485\n",
      "NCT01739673\n",
      "NCT02052934\n",
      "NCT02139163\n",
      "NCT02269761\n",
      "NCT02380352\n",
      "NCT02383680\n",
      "NCT02391909\n",
      "NCT02441699\n",
      "NCT02618655\n",
      "NCT02625129\n",
      "            _id     score\n",
      "0   NCT00000932 -0.034203\n",
      "1   NCT00086879 -0.034257\n",
      "2   NCT00094887 -0.133432\n",
      "3   NCT00095316 -0.115778\n",
      "4   NCT00096772 -0.068186\n",
      "..          ...       ...\n",
      "57  NCT02383680 -0.118365\n",
      "58  NCT02391909 -0.132734\n",
      "59  NCT02441699 -0.052484\n",
      "60  NCT02618655 -0.138762\n",
      "61  NCT02625129 -0.068747\n",
      "\n",
      "[62 rows x 2 columns]\n",
      "NCT00001776\n",
      "NCT00001777\n",
      "NCT00003562\n",
      "NCT00003859\n",
      "NCT00005666\n",
      "NCT00006087\n",
      "NCT00024908\n",
      "NCT00047801\n",
      "NCT00049543\n",
      "NCT00088569\n",
      "NCT00124761\n",
      "NCT00128986\n",
      "NCT00172575\n",
      "NCT00181272\n",
      "NCT00188279\n",
      "NCT00188435\n",
      "NCT00193765\n",
      "NCT00202176\n",
      "NCT00303901\n",
      "NCT00339066\n",
      "NCT00382707\n",
      "NCT00423332\n",
      "NCT00466947\n",
      "NCT00517959\n",
      "NCT00578084\n",
      "NCT00579852\n",
      "NCT00591838\n",
      "NCT00613041\n",
      "NCT00629460\n",
      "NCT00633035\n",
      "NCT00643318\n",
      "NCT00757120\n",
      "NCT00839488\n",
      "NCT00852644\n",
      "NCT00894569\n",
      "NCT00897650\n",
      "NCT00907543\n",
      "NCT00918320\n",
      "NCT00963651\n",
      "NCT00992966\n",
      "NCT01028417\n",
      "NCT01085864\n",
      "NCT01109147\n",
      "NCT01141842\n",
      "NCT01149187\n",
      "NCT01152463\n",
      "NCT01152788\n",
      "NCT01201824\n",
      "NCT01221493\n",
      "NCT01272622\n",
      "NCT01349400\n",
      "NCT01351870\n",
      "NCT01354912\n",
      "NCT01452971\n",
      "NCT01465425\n",
      "NCT01561729\n",
      "NCT01566682\n",
      "NCT01572571\n",
      "NCT01572584\n",
      "NCT01586611\n",
      "NCT01605214\n",
      "NCT01649752\n",
      "NCT01654887\n",
      "NCT01663155\n",
      "NCT01668459\n",
      "NCT01669135\n",
      "NCT01696292\n",
      "NCT01696968\n",
      "NCT01700257\n",
      "NCT01739881\n",
      "NCT01802125\n",
      "NCT01803542\n",
      "NCT01847209\n",
      "NCT01863069\n",
      "NCT01864538\n",
      "NCT01908933\n",
      "NCT01951846\n",
      "NCT02013063\n",
      "NCT02022371\n",
      "NCT02050724\n",
      "NCT02051868\n",
      "NCT02081105\n",
      "NCT02095808\n",
      "NCT02096445\n",
      "NCT02137291\n",
      "NCT02225392\n",
      "NCT02289144\n",
      "NCT02374242\n",
      "NCT02434107\n",
      "NCT02439086\n",
      "NCT02441478\n",
      "NCT02445183\n",
      "NCT02448225\n",
      "NCT02490059\n",
      "NCT02501668\n",
      "NCT02505750\n",
      "NCT02519972\n",
      "NCT02563691\n",
      "NCT02623712\n",
      "            _id     score\n",
      "0   NCT00001776 -0.403878\n",
      "1   NCT00001777 -0.393401\n",
      "2   NCT00003562 -0.077815\n",
      "3   NCT00003859 -0.029028\n",
      "4   NCT00005666 -0.088894\n",
      "..          ...       ...\n",
      "94  NCT02501668 -0.112678\n",
      "95  NCT02505750 -0.019799\n",
      "96  NCT02519972 -0.079713\n",
      "97  NCT02563691 -0.009969\n",
      "98  NCT02623712 -0.090326\n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-6fd2cc0831d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaseid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_ind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0maux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a9a70d3e17e8>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, caseid, docs)\u001b[0m\n\u001b[0;32m      8\u001b[0m         index = TfidfVectorizer(ngram_range=(\n\u001b[0;32m      9\u001b[0m             1, 1), analyzer='word', stop_words=None)\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcaseid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1834\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1836\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1837\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1220\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m_word_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;34m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;31m# handle stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#scores para as queries de teste\n",
    "\n",
    "p10_list=[]\n",
    "recall_list=[]\n",
    "ap_list=[]\n",
    "ndcg5_list=[]\n",
    "mrr_list=[]\n",
    "\n",
    "for caseid in cases_test:\n",
    "    VSM_bt_test = []\n",
    "    VSM_dd_test = []\n",
    "    VSM_bs_test = []\n",
    "    VSM_cr_test = []\n",
    "    \"\"\"\n",
    "    LMJM_bt_test = []\n",
    "    LMJM_dd_test = []\n",
    "    LMJM_bs_test = []\n",
    "    LMJM_cr_test = []\n",
    "    \"\"\"\n",
    "    y_test = []\n",
    "    case_rel = []\n",
    "    field_ind = 0\n",
    "    zs = []\n",
    "    aux = eval.relevance_judgments.loc[eval.relevance_judgments['query_id'] == int(\n",
    "        caseid)]\n",
    "    docs = aux['docid'].tolist()\n",
    "    # print(len(docs))\n",
    "    relevances = aux['rel'].tolist()\n",
    "    for rel in relevances:\n",
    "        if rel == 0:\n",
    "            y_test.append(rel)\n",
    "        elif rel == 1 or rel == 2:\n",
    "            y_test.append(1)\n",
    "    # print(len(relevances))\n",
    "    for docid in docs:\n",
    "        case_rel.append(ids.index(docid))\n",
    "    # print(len(case_rel))\n",
    "    for model in models:\n",
    "        for field in fields:\n",
    "            scores = model.search(caseid, field)\n",
    "            if(field_ind == 0):\n",
    "                aux = scores\n",
    "            for rel in case_rel:\n",
    "                value = scores[rel] if rel < len(scores) else aux[rel]\n",
    "                if field_ind == 0:\n",
    "                    VSM_bt_test.append(value[0])\n",
    "                elif field_ind == 1:\n",
    "                    VSM_dd_test.append(value[0])\n",
    "                elif field_ind == 2:\n",
    "                    VSM_bs_test.append(value[0])\n",
    "                elif field_ind == 3:\n",
    "                    VSM_cr_test.append(value[0])\n",
    "                \"\"\"\n",
    "                elif field_ind == 4:\n",
    "                    LMJM_bt_test.append(value[0])\n",
    "                elif field_ind == 5:\n",
    "                    LMJM_dd_test.append(value[0])\n",
    "                elif field_ind == 6:\n",
    "                    LMJM_bs_test.append(value[0])\n",
    "                elif field_ind == 7:\n",
    "                    LMJM_cr_test.append(value[0])\n",
    "                \"\"\"\n",
    "            field_ind += 1\n",
    "    # print(VSM_bt_test)\n",
    "    # print(len(VSM_dd_test))\n",
    "    # print(len(VSM_bs_test))\n",
    "    # print(len(VSM_cr_test))\n",
    "    # print(len(y_test))\n",
    "    for line in range(0, len(VSM_bt_test)):\n",
    "        #print(coefs[0] * VSM_bt_test[line] + coefs[1] * VSM_dd_test[line] + coefs[2] * VSM_bs_test[line] + coefs[3] * VSM_cr_test[line])\n",
    "        z = coefs[0]*VSM_bt_test[line]+coefs[1]*VSM_dd_test[line] + coefs[2]*VSM_bs_test[line]+coefs[3]*VSM_cr_test[line]\n",
    "        zs.append(z)\n",
    "\n",
    "    doc_ids=[]\n",
    "    for pos in case_rel:\n",
    "        doc_ids.append(ids[pos])\n",
    "\n",
    "    cand={'_id': doc_ids, 'score': zs}\n",
    "    results = pd.DataFrame(cand, columns = ['_id', 'score'])\n",
    "    \n",
    "    [p10, recall, ap, ndcg5, mrr] = eval.eval(results, caseid)\n",
    "    [precision_11point, recall_11point,\n",
    "        total_relv_ret] = eval.evalPR(results, caseid)\n",
    "\n",
    "    p10_list += [p10]\n",
    "    recall_list += [recall]\n",
    "    ap_list += [ap]\n",
    "    ndcg5_list += [ndcg5]\n",
    "    mrr_list += [mrr]\n",
    "    #print(zs)\n",
    "    candidates = {'query': caseid,\n",
    "                  'doc': case_rel,\n",
    "                  'VSM_bt': VSM_bt_test,\n",
    "                  'VSM_dd': VSM_dd_test,\n",
    "                  'VSM_bs': VSM_bs_test,\n",
    "                  'VSM_cr': VSM_cr_test,\n",
    "                  'Z': zs,\n",
    "                  'Y': y_test\n",
    "                  }\n",
    "    df = pd.DataFrame(candidates, columns=['query', 'doc',\n",
    "                                           'VSM_bt', 'VSM_dd', 'VSM_bs', 'VSM_cr', 'Z', 'Y'])\n",
    "\n",
    "    df.sort_values(by=['Z'], inplace=True, ascending=True)\n",
    "\n",
    "    #print(df.head(10))\n",
    "    #print(\"--------------------------------------------------------------------------------\")\n",
    "\n",
    "    #print(sorted(zs, reverse=False))\n",
    "print(np.mean(p10_list))\n",
    "print(np.mean(recall_list))\n",
    "print(np.mean(ap_list))\n",
    "print(np.mean(ndcg5_list))\n",
    "print(np.mean(mrr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recall_11point' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-4e7056230be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_11point\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_precision_11point\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'recall_11point' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recall_11point,avg_precision_11point/len(cases))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dba07cd5b14086a18474dc8785bfd16e6215fd6a835b09eec7fb218d0542f46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
